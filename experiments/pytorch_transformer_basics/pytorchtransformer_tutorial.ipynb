{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Tutorial, PytTorch\n",
    "Link: https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TorchData: https://github.com/pytorch/data\n",
    "# !pip install portalocker\n",
    "# !pip install torchdata\n",
    "# !pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShardingFilterIterDataPipe\n",
      " \n",
      "\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      " \n",
      "\n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      "\n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
      "\n",
      "['it', 'met', 'with', 'positive', 'sales', 'in', 'japan', ',', 'and', 'was']\n",
      "[24, 790, 18, 936, 1446, 6, 976, 2, 5, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([   24,   790,    18,   936,  1446,     6,   976,     2,     5,    10,\n",
       "          734,    19,    99,   780,     5,   424,   518,     3,    45,   320,\n",
       "            2,    24,   203,  5987,  1838,     2,   163,    18,    30,  2142,\n",
       "         1319,     6,   268,     4,    16,    85,     3,    24,    10,    44,\n",
       "         2527,    60,  3358,     5,    30,   264,   232,  4102,    93,     3,\n",
       "          192,     7,   482,  1446,     4,  3849,  3869,   304,     2,  3849,\n",
       "         3869,   881,    10,    39,  9642,     2,    38,     8,  2910,  4031,\n",
       "        10397,    18,     1,    67,    11,    15,  2142,  1319,    10,   155,\n",
       "            6,   669,     3,   757,     3,  2500,    62,   544,     7,     1,\n",
       "         2943,    18,     1,   369,     4,  3849, 20791,  1870,    17,     1,\n",
       "         1767,   116,     3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play with new functions\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "print(train_iter)\n",
    "\n",
    "# get single item one by one\n",
    "yielded_items = iter(train_iter)\n",
    "print(next(yielded_items))\n",
    "print(next(yielded_items))\n",
    "print(next(yielded_items))\n",
    "print(next(yielded_items))\n",
    "print(next(yielded_items))\n",
    "\n",
    "# test tokenizer\n",
    "tokens = tokenizer(next(yielded_items))\n",
    "print(tokens[:10])\n",
    "\n",
    "# test vocab\n",
    "vocab_result = vocab(tokens)\n",
    "print(vocab_result[:10])\n",
    "torch.tensor(vocab_result, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 20\n",
      "torch.Size([102499, 20])\n",
      "tensor([[ 3849,    12,   300,  6302,  3989,  1930, 10559,   451,     4,     7,\n",
      "             2,  1511, 10115,   942,  2439,   572,     1,    47,    30,  1990],\n",
      "        [ 3869,   315,    19,    29,   939,     2,    10,  2139,  4916, 16615,\n",
      "           235,     3,    13,     7,    24,    17, 13737,    97,  7720,     4],\n",
      "        [  881,    67,   807,  5402,     6,    38, 28188,    25,     2,    77,\n",
      "             7,  2394,    17,   516,    14, 16403,  3714,  4618,    12,  1108],\n",
      "        [    9,   196,  6041,   190,   218, 11776,    17,     1,  1200,     2,\n",
      "             0,    10,   591,    40,  6004,     2,    50,     3,  3131,  3781]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"batch size: {batch_size}\")\n",
    "print(train_data.shape)\n",
    "print(train_data[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence length is different from sentence length. It's similar to count of batches.\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28782\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "print(ntokens)\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        output = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        loss = criterion(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            # print(\"\\n\")\n",
    "            # print(targets.shape)\n",
    "            # print(targets[:20])\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            # print(output.shape)\n",
    "            # print(output_flat[:20])\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 44.51 | loss  8.13 | ppl  3383.42\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 30.46 | loss  6.24 | ppl   513.62\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 29.87 | loss  5.60 | ppl   269.78\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 30.38 | loss  5.29 | ppl   197.72\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 30.42 | loss  4.85 | ppl   127.72\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 30.65 | loss  4.47 | ppl    87.20\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 29.93 | loss  4.17 | ppl    64.41\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 30.01 | loss  3.98 | ppl    53.30\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 30.21 | loss  3.76 | ppl    43.09\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 30.65 | loss  3.64 | ppl    37.95\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 30.29 | loss  3.48 | ppl    32.51\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 30.24 | loss  3.46 | ppl    31.89\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 30.66 | loss  3.40 | ppl    29.83\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 30.34 | loss  3.27 | ppl    26.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 96.92s | valid loss  1.97 | valid ppl     7.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 30.25 | loss  3.06 | ppl    21.38\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 30.05 | loss  3.00 | ppl    20.08\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 30.84 | loss  2.84 | ppl    17.11\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 30.27 | loss  2.82 | ppl    16.72\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 30.18 | loss  2.70 | ppl    14.92\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 29.87 | loss  2.70 | ppl    14.90\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 30.15 | loss  2.70 | ppl    14.90\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 30.10 | loss  2.70 | ppl    14.91\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 30.64 | loss  2.60 | ppl    13.49\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 29.24 | loss  2.61 | ppl    13.61\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 29.17 | loss  2.53 | ppl    12.53\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 29.26 | loss  2.60 | ppl    13.52\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 29.20 | loss  2.61 | ppl    13.62\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 29.17 | loss  2.54 | ppl    12.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 94.12s | valid loss  1.70 | valid ppl     5.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 30.76 | loss  2.40 | ppl    11.00\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 30.95 | loss  2.39 | ppl    10.97\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 30.16 | loss  2.31 | ppl    10.08\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 30.14 | loss  2.29 | ppl     9.84\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 30.71 | loss  2.24 | ppl     9.40\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 30.18 | loss  2.27 | ppl     9.71\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 31.22 | loss  2.26 | ppl     9.54\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 29.27 | loss  2.37 | ppl    10.67\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 29.30 | loss  2.20 | ppl     9.04\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 29.22 | loss  2.35 | ppl    10.52\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 29.19 | loss  2.18 | ppl     8.85\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 29.23 | loss  2.33 | ppl    10.27\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 29.24 | loss  2.23 | ppl     9.30\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 29.15 | loss  2.18 | ppl     8.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 90.83s | valid loss  1.20 | valid ppl     3.30\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  1.17 | test ppl     3.21\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "target\n",
      "torch.Size([350])\n",
      "homarus the wolverines to = following to m been happened gammarus north men sweep scientology resolution mark ) a during\n",
      "output\n",
      "torch.Size([35, 10, 28782])\n",
      "homarus the wolverines sweep = following mark m been happened , north men front scientology resolution his ) a during\n",
      "data\n",
      "torch.Size([35, 10])\n",
      "= homarus gammarus = homarus gammarus , known as the european lobster or common lobster , is a species of <unk> lobster from the eastern atlantic ocean , mediterranean sea and parts of the black to the north west require considerable care . = = = <unk> = = = there is an airstrip which <unk> for regular flights from <unk> . = = culture and the arts = = michigan wolverines men ' s basketball team = the 2011 – 12 michigan wolverines men ' s basketball team represented the university of michigan during the 2011 – 12 ncaa division i men ' s support to sweep the area in front of the marine lines four japanese 37 mm ( 1 @ . @ 46 in ) anti @-@ tank guns destroyed or disabled three of them . after . = scientology in germany = the church of scientology has been present in germany since 1970 . german authorities estimate that there are 4 @ , @ 000 active scientologists in germany today the the following resolution that , having heard all available evidence regarding the charges against certain members of the native team , and having received an explicit denial of charges from the accused members and a states to mark the game ' s fifteenth anniversary . it included several new features , such as the ability to save game progress , a level select option , and an <unk> mode with 37 m ) diameter french burr <unk> is located on a <unk> frame , driven by the auxiliary engine via a fast and loose <unk> . = = <unk> = = william button 1819 @-@ having been a member of the unit , or a <unk> team . churchill received his <unk> in technological communications in 1974 and <unk> in communications theory in 1975 , both from <unk> state university which happened during filming . the depiction of the police attempting to shut down the video shoot due to safety concerns actually happened during filming , just as seen in the video . <unk> was\n",
      "\n",
      "\n",
      "target\n",
      "torch.Size([350])\n",
      ". papa season their of explanation spin rebecca now arrested it <unk> . burning scientology from <unk> button the following\n",
      "output\n",
      "torch.Size([35, 10, 28782])\n",
      ". fred season their of explanation spin sandy now arrested it <unk> . burning scientology from <unk> in the following\n",
      "data\n",
      "torch.Size([35, 10])\n",
      "sea . it is closely related to the american lobster , h . americanus . it may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( the papa <unk> sword dance may be of norse origin and bears similarities to the long sword dance of the north east of england . a description of the dance appears in the pirate by basketball season . the team played its home games in ann arbor , michigan at <unk> center for the <unk> consecutive year . it had a seating capacity of 12 @ , @ <unk> . abandoning their burning tanks , several of the disabled tanks ' <unk> were <unk> and killed by the japanese . one tank <unk> down an embankment into the <unk> river , drowning its crew . church of scientology gives a membership figure of around 12 @ , @ 000 . the church of scientology has encountered particular <unk> from the german press and government and occupies a precarious legal , satisfactory explanation from the management , we are of opinion that there are no facts before us <unk> the allegations . . . it is unlikely , given the attitude of the <unk> rugby union the spin <unk> move ( not originally implemented until sonic the hedgehog 2 ) . its screen is slightly <unk> in , and adapted to the gba ' s widescreen aspect ratio . the game 37 rebecca button <unk> ? ? richard button 18 ? ? <unk> william button 1860 @-@ 62 richard button ( jr ) 1860 @-@ 62 stephen <unk> 1862 @-@ 79 henry bryant 1879 @-@ 1920 , now the university of illinois at springfield . = = career = = in 1978 , churchill began working at the university of colorado boulder as an <unk> action officer in the university administration almost arrested following a confrontation with the police . according to avis , the events depicted in the video show what actually happened that day almost in real time , and that getting <unk> was\n",
      "\n",
      "\n",
      "target\n",
      "torch.Size([350])\n",
      "lb walter was 23 and the poor vincent he integral ) scott also 00 cultural <unk> reviews 1920 also part\n",
      "output\n",
      "torch.Size([35, 10, 28782])\n",
      "lb walter was 23 and the poor vincent he integral ) scott also 00 cultural <unk> reviews 1920 also part\n",
      "data\n",
      "torch.Size([35, 10])\n",
      "13 lb ) , and bears a conspicuous pair of claws . in life , the lobsters are blue , only becoming lobster red on cooking . mating occurs in the summer , producing eggs sir walter scott . the writer and journalist john sands lived on papa <unk> and <unk> for a while during the late nineteenth century . the writer , <unk> and musician , george p . it was also the team ' s <unk> straight season as a member of the big ten conference . fifth @-@ year head coach john <unk> led the team , alongside all @-@ big ten at 23 00 on 14 september , the remnants of the <unk> battalion conducted another attack on the same portion of the marine lines , but were repulsed . a final weak attack by the social and cultural position in germany . german courts have so far not resolved whether scientology should be accorded the legal status of a religious or <unk> community , and different courts have reached contradictory to the <unk> before their departure , that they would have dismissed the allegations if <unk> evidence had existed . the side was back to full strength following the return of their suspended players when received poor reviews , with a metacritic score of 33 percent the chief complaints concerned its poor conversion to the game boy advance ( resulting in a slow frame rate ) , remixed <unk> music <unk> vincent 1920 @-@ 24 george vincent 1920 @-@ 24 reference for above - = = culture and media = = a video of the restoration of the mill was produced by <unk> <unk> . . he also <unk> on american indian issues in the ethnic studies program . in 1990 , the university of colorado hired him as an associate professor , although he did not possess the academic an integral part of the plan . band manager paul mcguinness revealed in 2007 that much of the confrontation with the police was exaggerated the group were hoping to get shut down by the authorities\n",
      "\n",
      "\n",
      "target\n",
      "torch.Size([350])\n",
      "are . trey unit . side and = usually order carried peterson burke on the faced poor public required to\n",
      "output\n",
      "torch.Size([35, 10, 28782])\n",
      "are . secretly unit . side and = required order carried declares , of the faced poor public required <unk>\n",
      "data\n",
      "torch.Size([35, 10])\n",
      "which are carried by the females for up to a year before hatching into <unk> larvae . homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around s . peterson was brought up on papa <unk> . it is also the ' papa ' of <unk> ' s poem da sang o da papa men , now adopted as part of the players trey burke , tim <unk> , jr . and zack novak . burke was named big ten freshman of the year and was michigan ' s first associated press all @-@ american <unk> since <unk> unit on the evening of 15 september was also defeated . <unk> ' s unit of about 650 men attacked the marines at several locations on the west side of the <unk> perimeter . conclusions . the german domestic intelligence service has monitored the organization ' s activities . the german government does not recognize scientology as a religion . it views it as an abusive business <unk> as the side faced <unk> in <unk> . the <unk> <unk> their opponents five tries to two , and won 11 – 8 . the side ' s star player and half @-@ back , <unk> , and poor preservation of the original gameplay . = = = compilation releases = = = with its sequels for the genesis , sonic the hedgehog has been ported for a wide range of = = public access = = <unk> mill is open to the public on easter monday , the sunday of the late may and august bank holidays and the first sunday in july , august doctorate usually required for the position . the following year he was granted tenure in the communications department , without the usual six @-@ year <unk> period , after having been declined by the sociology in order to <unk> the music video , but the police continually gave them extensions for shooting the video . in the background of the video is a sign for the million dollar hotel ,\n",
      "\n",
      "\n",
      "target\n",
      "torch.Size([350])\n",
      "british tradition . about religion stayed and september political was isles , the 04 and in handheld . science rebuilt\n",
      "output\n",
      "torch.Size([35, 10, 28782])\n",
      "british tradition . about religion stayed and september political was isles , the 04 and in handheld is science rebuilt\n",
      "data\n",
      "torch.Size([35, 10])\n",
      "the british isles . = = description = = homarus gammarus is a large <unk> , with a body length up to 60 centimetres ( 24 in ) and weighing up to 5 – 6 <unk> tradition , as set to music by <unk> manson . the <unk> chorus chant , ' <unk> <unk> <unk> ! ' , is particularly striking . <unk> <unk> da horn o papa , <unk> 1998 . the team ' s season began with a preseason media day and practices in october 2011 . in february 2012 , michigan hosted espn ' s college <unk> for the first time in at about 04 00 on 14 september , two japanese companies attacked positions held by the 3rd battalion , 5th marine regiment ( 3 / 5 ) near the coast and were thrown back with a religion and believes that it pursues political goals that conflict with the values enshrined in the german constitution . this stance has been criticized , most notably by the u . s . government , stayed in <unk> when the team departed for <unk> . the side faced <unk> ' s bay , who were touring , in <unk> , and <unk> defeated them 13 – 2 . the home and handheld consoles and personal computers . it has appeared in sonic compilation ( 1995 ) for the genesis , sonic jam ( 1997 ) for the sega saturn and <unk> , sonic mega and september . it is also open at other times by prior arrangement . = <unk> mouth = <unk> mouth is the first segment of the 18th episode of the second season , or the and political science departments . he has long been interested in issues associated with the <unk> act , which broke up the communal reservation lands and assigned plots to individual households . <unk> with that which was rebuilt to create some interest , in case no one showed up at the film shoot . although the video is of a live performance , the audio used is from the studio\n",
      "\n",
      "\n",
      "target\n",
      "torch.Size([350])\n",
      "( <unk> game losses which ' ( overall the recorded 11 ! against . recognizes play 2002 episode federal version\n",
      "output\n",
      "torch.Size([35, 10, 28782])\n",
      "( <unk> game losses which ' ( overall the recorded 11 ! against . recognizes play 2002 episode federal version\n",
      "data\n",
      "torch.Size([35, 10])\n",
      "kilograms ( 11 – 13 lb ) , although the lobsters caught in lobster pots are usually 23 – 38 cm ( 9 – 15 in ) long and weigh 0 @ . @ 7 <unk> <unk> ! <unk> a hidden piece o water , <unk> <unk> <unk> ! <unk> da boat da tide @-@ <unk> <unk> , <unk> <unk> da <unk> is <unk> we <unk> <unk> <unk> fish is a game against ohio state . it was the eighth time a big ten team hosted the show , which began in 2005 . the team was in the national rankings all season and ended heavy losses . another japanese company captured a small ridge somewhat inland but was then pinned down by marine artillery fire throughout the day and took heavy losses before withdrawing on the evening of 14 , which recognizes scientology as a religion and has raised concerns about the violation of individual rights posed by sect <unk> . scientologists in germany face specific political and economic restrictions . they are barred <unk> ' play was praised by the press . . . the <unk> of the black passed with remarkable accuracy and <unk> between their legs , over their shoulders , under their arms and with collection ( 2002 ) , sonic mega collection plus ( 2004 ) , sonic ' s ultimate genesis collection ( 2009 ) for the xbox 360 and playstation 3 , and sonic classic collection ( 38th overall episode of the american animated television series <unk> <unk> . the episode was directed by andrew <unk> , and was written by walt <unk> , paul <unk> , and <unk> williams . <unk> was the federal government ' s first use of blood quantum to define individual membership in tribes , for what became known as the <unk> rolls . since re @-@ establishing self @-@ governments , @-@ recorded version of the song . the video won the grammy award for best performance music video at the 1989 grammy awards . = = = b @-@ sides = = = race against\n",
      "\n",
      "\n",
      "target\n",
      "torch.Size([350])\n",
      "2 , the . membership feet ) <unk> recognized was @ <unk> 2011 the in . for also tribes released\n",
      "output\n",
      "torch.Size([35, 10, 28782])\n",
      "2 , the . membership feet ) <unk> tribes was @ <unk> 2011 the membership . for also tribes released\n",
      "data\n",
      "torch.Size([35, 10])\n",
      "– 2 @ . @ 2 kg ( 1 @ . @ 5 – 4 @ . @ 9 lb ) . like other crustaceans , lobsters have a hard <unk> which they must shed <unk> , <unk> <unk> <unk> ! <unk> <unk> <unk> ! refers to the <unk> ' practice of rowing their open fishing boat out to sea until the high cliffs of <unk> were no longer visible as the 2011 – 12 big ten co @-@ champion with michigan state and ohio state . it had three victories over teams ranked in the top 10 at the time of the meeting ( september . the rest of <unk> ' s unit failed to find the marine lines and did not participate in the attack . = = aftermath = = at 13 05 on 14 september , from membership in some major political parties , and businesses and other employers use so @-@ called sect <unk> to expose a prospective business partner ' s or employee ' s association with the organization their feet . the side then faced canterbury on 17 august , who they <unk> 15 – 0 . the report published in the press said of the <unk> ' performance the play showed on 2010 ) for the nintendo ds . additionally , the game is an <unk> reward in the console versions of sonic <unk> . = = = downloadable releases = = = sonic the hedgehog has and <unk> also served as storyboard directors , and carson <unk> , william <unk> , and erik <unk> worked as storyboard artists . it originally aired on <unk> in the united states on september 21 <unk> recognized tribes have established their own criteria for enrollment as members , often related to descent from recognized historical lists , but less often requiring proofs of blood quantum . some of his published time was released on the 12 @-@ inch , cassette , and cd versions of the single . the song developed from the band ' s interest in urban funk , and was described by\n",
      "\n",
      "\n",
      "target\n",
      "torch.Size([350])\n",
      "order this @-@ led german afternoon available 2001 address edge to entailed ranked the federal was for . these as\n",
      "output\n",
      "torch.Size([35, 10, 28782])\n",
      "order this @-@ led german afternoon available 2001 address edge in personally memphis the federal was for . these as\n",
      "data\n",
      "torch.Size([35, 10])\n",
      "in order to grow , in a process called <unk> ( <unk> ) . this may occur several times a year for young lobsters , but decreases to once every 1 – 2 years for . this entailed the boat being some 96 kilometres ( 60 mi ) west of papa <unk> . the ' tide @-@ <unk> ' are increased swells of unusual size due to the combined action eighth @-@ ranked memphis , ninth @-@ ranked michigan state and sixth @-@ ranked ohio state ) . the team was undefeated at home until its last home game of the season . michigan lost <unk> led the survivors of his shattered brigade away from the ridge and deeper into the jungle , where they rested and tended to their wounded all the next day . <unk> ' s units . german federal and state interior ministers started a process aimed at banning scientology in late 2007 , but abandoned the initiative a year later , finding insufficient legal grounds . despite this , polls saturday afternoon was a fine exhibition of what several months of combination and practice will do . . . it must be admitted they were far and away too good for our local men . been available for all three major seventh @-@ generation video game consoles . it was part of the wii virtual <unk> at the service ' s 2006 introduction , and was released for the xbox , 2001 . the series follows the adventures and <unk> of the title character and his various friends in the underwater city of <unk> <unk> . in this episode , <unk> reads a bad word works address these issues , which he has interpreted as part of the federal government ' s policy of <unk> against native americans . in 1995 churchill discussed his views with david <unk> in an the edge as a kind of afro @-@ rhythmic piece and a study in rhythm . the bass riff in the song , inspired by the <unk> , was played by the edge , but\n",
      "\n",
      "\n",
      "target\n",
      "torch.Size([350])\n",
      "animals wind the then that the arcade a you from . against semifinals ordered most loose and <unk> could some\n",
      "output\n",
      "torch.Size([35, 10, 28782])\n",
      "animals wind the then most the arcade a you from . tide semifinals ordered favor loose and <unk> could some\n",
      "data\n",
      "torch.Size([35, 10])\n",
      "larger animals . the first pair of <unk> is armed with a large , asymmetrical pair of claws . the larger one is the <unk> , and has rounded <unk> used for crushing prey the of wind against tide . the <unk> final image of the piece is of the fishermen being led back home to papa by the ' scent o <unk> ' across the water . this is in the semifinals of the 2012 big ten conference tournament and bowed out in the second round of the 2012 ncaa tournament to end the season with a 24 @-@ 10 record . the team were then ordered to withdraw west to the <unk> river valley to join with <unk> ' s unit , a 6 mi ( 9 @ . @ 7 km ) march over difficult terrain . suggest that most germans favor banning scientology altogether . = = background = = scientology , founded in the early 1950s in the united states by l . ron hubbard and today claiming to be in the loose , in the <unk> , <unk> , passing , <unk> , or running they were very much indeed canterbury ' s superior . such runs as were made by <unk> at full live arcade and playstation network shortly afterwards . the game was released for the <unk> classic , <unk> video , and video @-@ capable <unk> <unk> models in 2007 and for apple ' s ios off a <unk> behind the <unk> <unk> , but does not know what it means . patrick explains to him that it is a sentence <unk> which is used when you want to talk fancy interview you could say that five hundred years ago was the basis of blood quantum in <unk> @-@ america . but in anglo @-@ america , while there was some preoccupation with it , it stemmed from some of clayton ' s unused bass parts . mullen ' s drum part was recorded in a single take . the song is primarily an instrumental piece but does contain some lyrics\n",
      "\n",
      "\n",
      "target\n",
      "torch.Size([350])\n",
      "is example the ' in , ( the not by the of school s 150 by compatible two formalized bono\n",
      "output\n",
      "torch.Size([35, 10, 28782])\n",
      "is example the ' 150 , ( the not by the of school s 150 by compatible start mosley bono\n",
      "data\n",
      "torch.Size([35, 10])\n",
      "other is the cutter , which has sharp inner edges , and is used for holding or tearing the prey . usually , the left claw is the <unk> , and the right is the an example of <unk> ' s ability to create a vivid sensual impression of a situation . an extra layer of meaning is added by the knowledge that da horn o papa collapsed in a won the school ' s first big ten conference championship since the 1985 – 86 season and had the school ' s best big ten record ( 13 – 5 ) since the 1993 – <unk> ' s troops began the march on the morning of 16 september . almost every soldier able to walk had to help carry the wounded . as the march progressed , the exhausted and represented in 150 countries , has been a very controversial new religious movement . its stated <unk> aim is to clear the planet , to bring about an enlightened age in which every individual has back , by madigan , <unk> , and w . <unk> , the passing of h . <unk> , f . <unk> , and all the backs as well as several forwards , the rushes service ( compatible <unk> and <unk> touch models ) in april 2009 . sonic the hedgehog became available on <unk> in september 2009 . in october 2010 , it was released as a microsoft windows . the two start using it in every sentence they speak . however , when mr . <unk> hears them , he warns the two never to use the word again or any of the was not formalized until the passage of the general <unk> act , mid @-@ 1880s . at that point they began to define indian as being someone who was <unk> and <unk> of at least inspired by bono ' s trip to ethiopia after live aid and his witnessing firsthand the famine in occurrence these lyrical references include bono singing in an ethiopian language and following it with the phrase\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "def decode_output(output: Tensor) -> List[str]:\n",
    "    output_flat = output.softmax(dim=2).argmax(dim=2).cpu().view(-1)\n",
    "    return [vocab.get_itos()[i] for i in output_flat.numpy()]\n",
    "\n",
    "def decode_tokens(target: Tensor) -> List[str]:\n",
    "    return [vocab.get_itos()[i] for i in target.cpu().numpy()]\n",
    "\n",
    "\n",
    "def run(model: nn.Module, eval_data: Tensor) -> None:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    j = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            if (j >= 10):\n",
    "                return\n",
    "            j += 1\n",
    "\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(\"target\")\n",
    "            print(targets.shape)\n",
    "            targets = decode_tokens(targets)\n",
    "            print(\" \".join(targets[:20]))\n",
    "\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "\n",
    "            print(\"output\")\n",
    "            print(output.shape)\n",
    "            output = decode_output(output)\n",
    "            print(\" \".join(output[:20]))\n",
    "\n",
    "            print(\"data, transposed view\")\n",
    "            print(data.shape)\n",
    "            data_copy = decode_tokens(data.t().contiguous().view(-1))\n",
    "            print(\" \".join(data_copy))\n",
    "\n",
    "run(model, val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local test\n",
    "def prepare_input(input: str) -> Tensor:\n",
    "    return torch.tensor(vocab(tokenizer(input)), dtype=torch.long).t()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor:  tensor([185,  11,  15,  74, 857, 812, 524,   8, 117,   3], device='mps:0')\n",
      "input shape:  torch.Size([10])\n",
      "input:  tensor([185,  11,  15,  74, 857, 812, 524,   8, 117,   3], device='mps:0')\n",
      "\n",
      "\n",
      "after padding torch.Size([10, 1])\n",
      "vocab length:  28782\n",
      "output size:  torch.Size([10, 1, 28782])\n",
      "tensor([10.6758,  9.0736, 10.2610, 10.4223,  8.4606], device='mps:0')\n",
      "\n",
      "\n",
      "softmax-output size:  torch.Size([10, 1])\n",
      "\n",
      "\n",
      "tensor([ 11,  15,  74, 857, 812, 524,   8, 117,   3,   3])\n",
      "decoded output:  ' s up ? give me a number . .\n"
     ]
    }
   ],
   "source": [
    "q = prepare_input(\"what's up? give me a number.\").to(\"mps\")\n",
    "print(\"input tensor: \", q)\n",
    "print(\"input shape: \", q.shape)\n",
    "print(\"input: \", q)\n",
    "print(\"\\n\")\n",
    "\n",
    "# q = train_data[0]\n",
    "# pad_count = 20 - q.size(0)\n",
    "# padding\n",
    "# pad = torch.nn.ConstantPad1d((0, pad_count), 3)\n",
    "# q = pad(q)\n",
    "q.unsqueeze_(1)\n",
    "print(\"after padding\", q.shape)\n",
    "# print(\"\\n\")\n",
    "\n",
    "model.eval()  # turn on evaluation mode\n",
    "with torch.no_grad():\n",
    "    output = model(q)\n",
    "    print(\"vocab length: \", len(vocab))\n",
    "    print(\"output size: \", output.shape)\n",
    "    print(output[0,0,:5])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # softmax to get a prob\n",
    "    from torch.nn import Softmax\n",
    "    # output = Softmax(dim=2)(output)\n",
    "    output = output.softmax(dim=2).argmax(dim=2)\n",
    "    print(\"softmax-output size: \", output.shape)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # decode to string\n",
    "    index_to_string = vocab.get_itos()\n",
    "    output = output.to(\"cpu\").view(-1)\n",
    "    print(output)\n",
    "    output_str = \" \".join([index_to_string[x] for x in output])\n",
    "    print(\"decoded output: \", output_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Diffusion",
   "language": "python",
   "name": "diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
